# azure-data-pipeline
End-to-End Data Ingestion and Transformation through Azure Data Engineering
Authors: Arpan Ray, Mainak Mukherjee, Trishita Mukherjee

This repository contains a comprehensive project demonstrating an *end-to-end data engineering pipeline* built on the Microsoft Azure ecosystem.  
It showcases data ingestion, transformation, and storage using modern cloud-based tools and best practices.

# Project Contents
- End-to-End Data Ingestion and Transformation through Azure Data Engineering.pdf  
  Complete technical documentation of the project.

# Technologies Used
- Azure Data Factory – Orchestrated data ingestion pipelines from multiple sources.
- Azure Databricks (PySpark) – Performed scalable data transformation and cleaning.
- Delta Lake – Implemented reliable and versioned data storage.
- Azure Storage – Used for storing raw and processed data.
- GitHub – Version control and project hosting.

# Key Features
- Automated ETL (Extract, Transform, Load) data pipeline.
- Scheduled workflows with monitoring and logging.
- Modular design for scalability and maintainability.
- Cloud-based architecture ensuring high availability.



# Learning Outcomes
- Practical experience in building data engineering solutions on cloud.
- Understanding of pipeline orchestration and big data processing.
- Hands-on skills in Azure services and distributed data processing with PySpark.
